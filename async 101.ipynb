{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# making sync funcs async\n",
    "\n",
    "Trying out how to use async with I/O bound functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I have a simple slow function, which uses a non async library, requests, to grab a url and return the len of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21 ms, sys: 2 ms, total: 23 ms\n",
      "Wall time: 763 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10689"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = [\"google.com\", \"bing.com\", \"amazon.com\", \"google.com\", \"microsoft.com\"]*100\n",
    "\n",
    "def get_url_size(url=\"google.com\"):\n",
    "    r = requests.get(\"http://\"+ url)\n",
    "    return len(r.text)\n",
    "\n",
    "%time get_url_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost a second! I've cut down the number of urls to 20 so it doesn't take forever:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 860 ms, sys: 82 ms, total: 942 ms\n",
      "Wall time: 37.3 s\n",
      "got 20 first 10 results: [10708, 107607, 2671, 10701, 160820, 10672, 107607, 471175, 10696, 160815]\n"
     ]
    }
   ],
   "source": [
    "%time results = [get_url_size(url) for url in urls[:20]]\n",
    "print(f\"got {len(results)} first 10 results:\", results[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is taking forever, or 30 seconds to fetch 20 urls. Now with true async code we should be looking at a fraction of this time, depending on how many threads are running. \n",
    "\n",
    "Now, I should be able to rewrite `get_url_size` like so:\n",
    "\n",
    "```\n",
    "async def get_url_size(url=\"google.com\"):\n",
    "    r = await requests.get(\"http://\"+ url)\n",
    "    return len(r.text)\n",
    "```\n",
    "\n",
    "But this fails, as the function being called, `requests.get` in this case, needs to be async enabled. Since its old school code, it holds on to the CPU for dear life and doesn't let go until it returns. \n",
    "\n",
    "So we need to use threads to run it in parallel:\n",
    "\n",
    "Below, I use the default threads executor to spawn multiple threads to run the non async function above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got 20 first 10 results: [10691, 107607, 397539, 10666, 160810, 10675, 107607, 2671, 10720, 160810]\n",
      "CPU times: user 926 ms, sys: 120 ms, total: 1.05 s\n",
      "Wall time: 4.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "async def main():\n",
    "\n",
    "    loop = asyncio.get_event_loop()\n",
    "    futures = [loop.run_in_executor(None, get_url_size, url) for url in urls[:20]]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for response in await asyncio.gather(*futures):\n",
    "        results.append(response)\n",
    "    \n",
    "    return results\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "results = loop.run_until_complete(main())\n",
    "print(f\"got {len(results)} first 10 results:\", results[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is heaps faster, took 4 seconds to get 20 urls. Since grabbing one url takes just a second, theoritcally this whole op could be as fast as 1 second + overhead time. \n",
    "\n",
    "The reason for the fast but not super fast is the [default executor](https://docs.python.org/3/library/asyncio-eventloop.html#executor):\n",
    "\n",
    ">  If max_workers is None or not given, it will default to the number of processors on the machine, multiplied by 5, assuming that ThreadPoolExecutor is often used to overlap I/O instead of CPU work and the number of workers should be higher than the number of workers for ProcessPoolExecutor.\n",
    "\n",
    "Now, I'm doing I/O bound ops, so the internet says:\n",
    "\n",
    "- For CPU-bound workloads: use ProcessPoolExecutor\n",
    "- For I/O-bound workloads: use ThreadPoolExecutor\n",
    "\n",
    "So to go faster we can specify max threads for ThreadPoolExecutor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'concurrent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/data/data/com.termux/files/usr/lib/python3.6/asyncio/base_events.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Event loop stopped before Future completed.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36mmain_2\u001b[0;34m(num_threads)\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'concurrent' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "async def main_2(num_threads=100):\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "\n",
    "        loop = asyncio.get_event_loop()\n",
    "        futures = [loop.run_in_executor(executor, get_url_size, url) for url in urls]\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for response in await asyncio.gather(*futures):\n",
    "            results.append(response)\n",
    "\n",
    "    return results\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "results = loop.run_until_complete(main_2())\n",
    "print(f\"got {len(results)} first 10 results:\", results[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here the second version is faster, though in the same ballpark. Lets see what tinkering with the number of threads does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got 500 first 10 results: [2470, 105873, 2671, 2470, 146685, 2470, 105245, 2671, 2470, 146581]\n",
      "CPU times: user 16.4 s, sys: 1.83 s, total: 18.2 s\n",
      "Wall time: 24.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "loop = asyncio.get_event_loop()\n",
    "results = loop.run_until_complete(main_2(200))\n",
    "print(f\"got {len(results)} first 10 results:\", results[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suprisingly, not much from doubling threads from 100 to 200. So really, most of the time the default executor is fine for IO bound ops. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
